import numpy as np
import random

# 初始化矩阵
Q = np.zeros((6, 6))
Q = np.matrix(Q)

# 回报矩阵R，-1时代表此路不通，100代表目的地
# R的行代表State，列代表Action
# 第0行，第4列的值为0表示：当处于状态0时，执行前往状态4的Action所带来的回报是0
# 第1行，第5列的值为100表示：当处于状态1时，执行前往状态5的Action所带来的回报是100（因为目的地是5）
R = np.matrix([[-1, -1, -1, -1, 0, -1],
               [-1, -1, -1, 0, -1, 100],
               [-1, -1, -1, 0, -1, -1],
               [-1, 0, 0, -1, 0, -1],
               [0, -1, -1, 0, -1, 100],
               [-1, 0, -1, -1, 0, 100]])

# 设立学习参数
γ = 0.8

# 训练，共进行2000次
for i in range(2000):
    # 每一个训练,首先随机初始化为一种状态
    state = random.randint(0, 5)
    while True:
        # 当前状态下所有可执行动作（所有可以到达的状态，也可以说可以到达的位置）
        r_pos_action = []
        for action in range(6):
            # >=0 的动作才可以选择
            if R[state, action] >= 0:
                r_pos_action.append(action)
        # 在所有可以选择的动作（到达的状态，或者达到的位置）中随机选择一个
        next_state = r_pos_action[random.randint(0, len(r_pos_action) - 1)]
        # 更新Q表，Q表的行代表当前的状态（位置），列代表下一个状态（位置）
        # Q表是QLearning最终要学习到的东西，可以看作是Agent学到的经验
        # 训练结束后，Agent会根据Q表选择当前状态下，值最大的下一个状态，直到到达目的状态（目的地）
        # 假设当前state=3，下一个状态随机到了next_state=4
        # 更新过程Q[3, 4] = R[3, 4] + γ * (Q[4]).max()
        # Q[4].max()其实是{Q[4][0], Q[4][1], Q[4][2], Q[4][3], Q[4][4], Q[4][5]}中最大的一个
        # γ是折现因子,我是这么理解的：
        # Q(s1) <-- R(s2) + γ*Q(s2) <-- R(s2) + γ*(R(s3) + γ*R(s4)) <-- R(s2) + γ*(R(s3) + γ*(R(s4) + γ*( ...
        #       <-- R(s2) + γ*R(s3) + γ*γ*R(s4) + γ*γ*γ*R(s5) + ...
        # 最新的Q表来源于当前的R（回报）和当前的Q（由经验所预估的回报），γ是小于1的，所以Q有折损
        # 当前的Q表又是来源于过去的R和过去的Q，由于每次都乘了个小于1的γ，所以越老的R和Q折损得越严重
        # 所以最新的Q表（关于如何选择下一步的经验）的内容一部分来自当前的现状回报R，一部分来自于当前的Q
        # （.max()意为过去的经验指导下预估得到的最好的回报）
        # 但是过去的经验不一定是最靠谱的，所以给它加一个折现因子
        Q[state, next_state] = R[state, next_state] + γ * (Q[next_state]).max()
        print(f'i: {i}, Q[{state}, {next_state}] = {R[state, next_state]} + {γ} * ({Q[next_state]}) = {Q[state, next_state]}')
        # 当前状态前进到刚才选择的那个
        state = next_state
        # 循环，直到达到目标状态（到达目的地），本次训练结束，进行下一次训练
        if state == 5:
            break
# Q表是关于如何选择下一步的经验,当处于某一状态i时,会查询Q[i]这一行,找到最大值Q[i][j],j就是下一个状态
print(Q)